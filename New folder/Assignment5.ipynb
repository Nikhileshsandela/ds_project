{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "IDLAssignment5_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkK_haKPCKO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Amit Kumar\n",
        "# Nikhilesh\n",
        "# Shivam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H9AHQIS_WsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python prepare_data.py shakespeare.txt skp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQnUSUG8AdLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcC99_-c_WsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1ot1KFZ_WsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create weights variables\n",
        "def make_weights(i, o, initializer):\n",
        "    return tf.Variable(initializer(shape=[i, o], dtype=tf.float32))\n",
        "\n",
        "# create bias variables\n",
        "def make_bias(k, initializer):\n",
        "    return tf.Variable(initializer(shape=[k], dtype=tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK4kfrSV_Wsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN model\n",
        "\n",
        "# tf.Variable(\n",
        "#     initial_value=None, trainable=None, validate_shape=True, caching_device=None,\n",
        "#     name=None, variable_def=None, dtype=None, import_scope=None, constraint=None,\n",
        "#     synchronization=tf.VariableSynchronization.AUTO,\n",
        "#     aggregation=tf.compat.v1.VariableAggregation.NONE, shape=None\n",
        "# )\n",
        "\n",
        "w_range = 0.1\n",
        "i_layer = vocab_size\n",
        "h_layer = 512\n",
        "o_layer = vocab_size\n",
        "batch_size = 128\n",
        "batch_seq_length = 200\n",
        "\n",
        "# data = data.shuffle(60000)\n",
        "data = data.batch(batch_size)\n",
        "# data = data.repeat()\n",
        "\n",
        "w_xh = make_weights(i_layer, h_layer, tf.initializers.RandomUniform(minval=-w_range, maxval=w_range))\n",
        "w_hh = make_weights(h_layer, h_layer, tf.initializers.RandomUniform(minval=-w_range, maxval=w_range))\n",
        "\n",
        "b_h = make_bias(h_layer, tf.initializers.constant(0.001))\n",
        "\n",
        "w_ho = make_weights(h_layer, o_layer, tf.initializers.RandomUniform(minval=-w_range, maxval=w_range))\n",
        "\n",
        "b_o = make_bias(o_layer, tf.initializers.constant(0.001))\n",
        "\n",
        "\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc4I4rUv_Wsj",
        "colab_type": "code",
        "colab": {},
        "outputId": "80389854-f79a-49d8-9c7e-72b90c6399cf"
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (None, 200), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrYi3Vni_Wsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('w_xh : {}\\n'.format(w_xh))\n",
        "# print('w_hh : {}\\n'.format(w_hh))\n",
        "# print('b_h : {}\\n'.format(b_h))\n",
        "# print('w_ho : {}\\n'.format(w_ho))\n",
        "# print('b_o : {}\\n'.format(b_o))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0I3GtMf_Wst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_conversion(data, vocab_size):\n",
        "    return tf.one_hot(indices=data,depth=vocab_size);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87-DcJx__Wsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some more model :(\n",
        "\n",
        "def rrn_model(inp, h):\n",
        "    h = tf.nn.tanh(tf.matmul(h, w_hh) + tf.matmul(inp, w_xh) + b_h)\n",
        "    o = tf.nn.softmax(tf.matmul(h, w_ho) + b_o)\n",
        "    return h, o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6pVFoR-_Ws2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.constant(\n",
        "#     value, dtype=None, shape=None, name='Const'\n",
        "# )\n",
        "\n",
        "train_steps = 2000\n",
        "\n",
        "# @tf.function\n",
        "# def very_difficult_function(batch_size, batch_seq_length):\n",
        "for step, batch_seq in enumerate(data):\n",
        "#     print('batch_seq : {}'.format(batch_seq))\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        loss = tf.constant(0, dtype=tf.float32, shape=None)\n",
        "#         print('Initial Loss : {}'.format(loss))\n",
        "        h = tf.zeros([batch_size, h_layer])\n",
        "#         print('Initial Hidden State : {}'.format(h))\n",
        "        \n",
        "        for seq in tf.range(batch_seq_length - 1):\n",
        "#             print('Sequence : {}'.format(seq))\n",
        "            \n",
        "            train_data = batch_seq[:, seq]\n",
        "            # next character as ground truth\n",
        "            ground_truth = batch_seq[:, seq+1] \n",
        "            inp = one_hot_conversion(train_data, vocab_size)\n",
        "            h, o = rrn_model(inp, h)\n",
        "            xent = loss_fn(ground_truth, o)\n",
        "            loss = loss + xent\n",
        "        loss = loss / (batch_seq_length -1)\n",
        "#         print('Loss : {}'.format(loss))\n",
        "    varis = [w_xh, w_hh, b_h, w_ho, b_o]\n",
        "    grads = tape.gradient(xent, varis)\n",
        "    opt.apply_gradients(zip(grads, varis))\n",
        "    if not step % 100:\n",
        "        print(\"Step: {} Loss: {}\\n\".format(step, xent))\n",
        "        \n",
        "    if step > train_steps:\n",
        "        break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFeRlwN7_Ws5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = 2000\n",
        "\n",
        "for step, batch_seq in enumerate(data):\n",
        "    \n",
        "    if step > train_steps:\n",
        "        break\n",
        "    \n",
        "    xent = very_difficult_function(batch_size, batch_seq)\n",
        "    \n",
        "    \n",
        "    if not step % 100:\n",
        "        print(\"Step: {} Loss: {}\\n\".format(step, xent))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiun-jxM_Ws7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}