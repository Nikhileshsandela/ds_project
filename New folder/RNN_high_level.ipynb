{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 prepare_data2.py shakespeare_input.txt shake \\\\n\\\\n+ -m 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from prepare_data2 import parse_seq\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 128\n",
    "seq_len = 500\n",
    "data = tf.data.TFRecordDataset(\"shake.tfrecords\")\n",
    "data = data.map(lambda x: parse_seq(x))\n",
    "data = data.shuffle(46000).padded_batch(128, seq_len, drop_remainder=True).repeat()\n",
    "\n",
    "vocab = pickle.load(open(\"shake_vocab\", mode=\"rb\"))\n",
    "vocab_size = len(vocab)\n",
    "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<S>HELENA:\n",
      "And I am sick when I look not on you.</S><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "for ind, thing in enumerate(data):\n",
    "    inds = thing[2].numpy()\n",
    "    to_chars = \"\".join([ind_to_ch[ind] for ind in inds])\n",
    "    print(ind)\n",
    "    print(to_chars)\n",
    "    print()\n",
    "    if input() == 'q':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 512\n",
    "\n",
    "model_a = tf.keras.Sequential()\n",
    "model_a.add(tf.keras.Input(shape=(500,vocab_size), batch_size=128))\n",
    "model_a.add(tf.keras.layers.SimpleRNN(n_h, return_sequences=True, stateful=True))\n",
    "model_a.add(tf.keras.layers.Dense(vocab_size))\n",
    "\n",
    "\n",
    "steps = 20*35000 // bs\n",
    "opt = tf.optimizers.Adam()\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model_a.build()\n",
    "\n",
    "all_vars = [vars for vars in model_a.trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def run_rnn_on_seq(seq_batch):\n",
    "    model_a.reset_states()\n",
    "    with tf.GradientTape() as tape:\n",
    "        mask_len = [] #tf.TensorArray(tf.float32,size=128)\n",
    "        for seq in seq_batch:\n",
    "          mask_len.append(tf.math.count_nonzero(seq, dtype=tf.float32)-1,)\n",
    "\n",
    "        mask_len = tf.stack(mask_len)\n",
    "        mask = tf.sequence_mask(mask_len, seq_len, dtype=tf.float32)\n",
    "\n",
    "        oh_seq  = tf.one_hot(seq_batch, vocab_size, axis=-1)\n",
    "        y_actual = tf.roll(seq_batch, -1, 1)\n",
    "\n",
    "        logits = model_a(oh_seq)\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(y_actual, logits)\n",
    "        \n",
    "        losses = losses * mask\n",
    "        losses = tf.reduce_sum(losses, axis=1)\n",
    "\n",
    "        \n",
    "        xcent = tf.reduce_mean(tf.math.divide(losses,mask_len))\n",
    "\n",
    "\n",
    "    grads = tape.gradient(xcent, all_vars)\n",
    "    \n",
    "    # this is gradient clipping\n",
    "    glob_norm = tf.linalg.global_norm(grads)\n",
    "    grads = [g/glob_norm for g in grads]\n",
    "    \n",
    "    opt.apply_gradients(zip(grads, all_vars))\n",
    "\n",
    "    return xcent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 4.267581939697266\n",
      "\n",
      "Step: 100 Loss: 2.5142102241516113\n",
      "\n",
      "Step: 200 Loss: 2.157288074493408\n",
      "\n",
      "Step: 300 Loss: 2.076068878173828\n",
      "\n",
      "Step: 400 Loss: 2.005401134490967\n",
      "\n",
      "Step: 500 Loss: 2.0001754760742188\n",
      "\n",
      "Step: 600 Loss: 1.908968448638916\n",
      "\n",
      "Step: 700 Loss: 1.843993902206421\n",
      "\n",
      "Step: 800 Loss: 1.8664155006408691\n",
      "\n",
      "Step: 900 Loss: 1.8279231786727905\n",
      "\n",
      "Step: 1000 Loss: 1.75571870803833\n",
      "\n",
      "Step: 1100 Loss: 1.7242125272750854\n",
      "\n",
      "Step: 1200 Loss: 1.7182177305221558\n",
      "\n",
      "Step: 1300 Loss: 1.6788966655731201\n",
      "\n",
      "Step: 1400 Loss: 1.6467390060424805\n",
      "\n",
      "Step: 1500 Loss: 1.629185676574707\n",
      "\n",
      "Step: 1600 Loss: 1.6283624172210693\n",
      "\n",
      "Step: 1700 Loss: 1.562261939048767\n",
      "\n",
      "Step: 1800 Loss: 1.5526436567306519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, seqs in enumerate(data):\n",
    "    xent_avg = run_rnn_on_seq(seqs)\n",
    "\n",
    "    if not step % 100:\n",
    "        print(\"Step: {} Loss: {}\".format(step, xent_avg))\n",
    "        print()\n",
    "    \n",
    "    if xent_avg < 1.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model for generating text\n",
    "\n",
    "layers_gen = [tf.keras.Input(shape=(1,vocab_size), batch_size=1),\n",
    "          tf.keras.layers.SimpleRNN(n_h, return_sequences=True, stateful=True),\n",
    "          tf.keras.layers.Dense(vocab_size)]\n",
    "\n",
    "model_gen = tf.keras.Sequential(layers_gen)\n",
    "\n",
    "model_gen.build()\n",
    "\n",
    "model_gen.set_weights(model_a.get_weights())\n",
    "\n",
    "#model_gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUCLET:\n",
      "Ay. God, my Lord Padour?\n",
      "\n",
      "\n",
      "Soothir:\n",
      "Paintrube, that hath wit I thas toul failf\n",
      "Lorid?\n",
      "\n",
      "\n",
      "Secood Lord:\n",
      "Bornis, thou have your lodosu art thoucous; but fol ow you whil.\n",
      "\n",
      "\n",
      "LORW:\n",
      "Ay hay wouth whore Kith out.\n",
      "\n",
      "\n",
      "Lecauther:\n",
      "No 'et a mull, Gadiely laid And selbard nover a yough murn\n",
      "Cambarees ow\n",
      "the shill should't cause thay a is duin surstee tous medly.\n",
      "\n",
      "\n",
      "MIDTRESS POGE:\n",
      "Ge weed as that whith woman you? hear meen gard firraie:\n",
      "Whan Rous nogring bad, whore more that a wit comes free me;\n",
      "The bud, I am extervery of there in told ma do heave.\n",
      "\n",
      "\n",
      "GLOUCESTER:\n",
      "Whather, my lord, im to treeary is'd-ugood all lady cay for Cias\n",
      "Nor the word in it shill ho.\n",
      "\n",
      "\n",
      "KING HENRY V:\n",
      "Ally, as I avile oresban, what home, you bay mash mutitifron he is ho'e to by\n",
      "Ald dive go shay your tore.\n",
      "\n",
      "\n",
      "TOMIRSA:\n",
      "Four saMyssa shout hand\n",
      "go a spery sork, I mot lo' that I so:\n",
      "go ole courte my liers and lofe.\n",
      "Chese he shell me dowercio?\n",
      "\n",
      "\n",
      "HecLar:\n",
      "We liver, I am no, whing wioke hath noth'st at a evely tood.\n",
      "\n",
      "\n",
      "CASSIUS:\n",
      "Why, sird the Ouroor a mors warkidre them it fal ho, aff a you?\n",
      "\n",
      "\n",
      "LEONATO:\n",
      "I lind hy regal, a tomer it your patter's.\n",
      "\n",
      "\n",
      "Goot:\n",
      "The quith's saif anca brace cores out. Cimusch, for s are bait, hea thou? nothing. Soke wime to yes to thou brouden'l waars Dooprert of the?\n",
      "\n",
      "\n",
      "OIG BRANDOLE:\n",
      "What do hencunt sim?\n",
      "\n",
      "\n",
      "TOTROBEOT:\n",
      "You' murtow, goor wird peace! Fail wit.\n",
      "\n",
      "\n",
      "SHALLOW:\n",
      "Is!\n",
      "\n",
      "\n",
      "PERICLES:\n",
      "Ant theu soad, the fome of Doy this diast recall oo tut lady.\n",
      "\n",
      "\n",
      "OMI IAGO:\n",
      "Priroue it your goad notl tarket should rethmes crotking! I have hie pray Mame morrol.\n",
      "\n",
      "\n",
      "OPIEL:\n",
      "Und that's the lorn,--\n",
      "\n",
      "\n",
      "OTHELLO:\n",
      "Whuth shall you this?\n",
      "\n",
      "\n",
      "First Sirvies:\n",
      "Weye maray, thure your lary our foolfuredthins.\n",
      "\n",
      "\n",
      "BARDOLPH:\n",
      "What prayses' wall.\n",
      "\n",
      "\n",
      "EDgARD:\n",
      "Cleese meint.\n",
      "\n",
      "\n",
      "CROBWIS:\n",
      "\n",
      "\n",
      "SeLond Sofainis:\n",
      "Would bring the may: come ser must on comman'B homay the with bileved I have vonest they.\n",
      "\n",
      "\n",
      "Prive be queer There:\n",
      "Cinfeclly the princele so, sir.\n",
      "\n",
      "\n",
      "IUGBELL:\n",
      "IZ's your forsmire, Sir.\n",
      "'Twe hearts make you right, when deack, all world your uffeingl.\n",
      "\n",
      "\n",
      "MISCRESPORO:\n",
      "Lotys?\n",
      "\n",
      "\n",
      "FERDINAND:\n",
      "From e fournle wexplion?\n",
      "\n",
      "\n",
      "MIRO DA DORT:\n",
      "It itsala! Gooy gouct;\n",
      "I to and this besous now fremish ackes then, where's phated\n",
      "Seldor of my, comes, your from mervice?\n",
      "\n",
      "\n",
      "CLAVENCO:\n",
      "\n",
      "\n",
      "LOUB PEGERIUS:\n",
      "I cave my endarion, and is days the offtroh or;\n",
      "What trusstin that lack to oppe--olur.\n",
      "\n",
      "\n",
      "ARGEANT:\n",
      "Youke.\n",
      "\n",
      "\n",
      "PROTEUS:\n",
      "Why yiak tous thou' the nower of the QUurtust spaen a menca dowich thy broy t'ol master is fail no gorou.\n",
      "\n",
      "\n",
      "COUSTRANIE:\n",
      "I wala deserouno tham.\n",
      "\n",
      "\n",
      "BONACRIO:\n",
      "Qaintely lefess que lentlannty your she buge, I' lay'd were would fay thyo,\n",
      "Ay meed thee have ot,\n",
      "Mate thou, you tran 't thougnt maakine?\n",
      "\n",
      "\n",
      "GREMIO:\n",
      "Fillow him, is it do, sir.\n",
      "\n",
      "\n",
      "PRISCEES:\n",
      "And whirents him shearve tham diend!\n",
      "Whit faist you swey.\n",
      "\n",
      "\n",
      "EEGRO:\n",
      "Verin, my Ladiencel, mirdieed they, my sowr,\n",
      "Bull fresthers sir?\n",
      "\n",
      "\n",
      "KENT:\n",
      "Weath aby the mounters, sir.\n",
      "\n",
      "\n",
      "CASSIUS:\n",
      "Will this?\n",
      "\n",
      "\n",
      "Second Sitringle:\n",
      "Soom Sompreance here, and cat this haws: hame of the quistary purded ressoin to feeth; I'll mind entre; no dowire.\n",
      "\n",
      "\n",
      "FMORTES:\n",
      "Abex, With him to Dtand, my good lord.\n",
      "\n",
      "\n",
      "INOS:\n",
      "Then whilechoukn for this aponerth go dape liemorous my his coudre's fair?\n",
      "\n",
      "\n",
      "MARIA:\n",
      "O leprice was; lid seat of theer a too fith the go stow? amas;\n",
      "I' would dinnad, olis.\n",
      "\n",
      "\n",
      "LANETES:\n",
      "\n",
      "\n",
      "JAQUESSTIA:\n",
      "You canst by of Preckamen do so\n",
      "ere and suptword; and therefore which were ancenter the world sar\n",
      "Thme wifmin's sweet his maners and gome.\n",
      "\n",
      "\n",
      "IAGO:\n",
      "I'll engever dowel it, the parmy as honour?\n",
      "The covs a good lord's to faurn there's heartion?\n",
      "\n",
      "\n",
      "Missee--o'eliend Caysan, to Firscise it, mastrrison my curseind with pay it rofairvoly afly:\n",
      "I shirl father tree frem much ereverw in; for it is my dears in your radzar.\n",
      "\n",
      "\n",
      "SPREDCK:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gen_seq(n_seq):        \n",
    "  for _ in range(n_seq):\n",
    "    char = 1      \n",
    "    txt = []\n",
    "    while not char == 2:\n",
    "      gen = tf.Variable([[tf.one_hot(char, vocab_size)]])\n",
    "      out = model_gen(gen)\n",
    "      probs = tf.reshape(tf.nn.softmax(out),[-1]).numpy()\n",
    "      char = np.random.choice(vocab_size, p=probs)      \n",
    "      txt.append(char)\n",
    "      if char == 2:\n",
    "        model_gen.reset_states()\n",
    "        break\n",
    "\n",
    "    print(\"\".join([ind_to_ch[ind] for ind in txt]).replace(\"</S>\", \"\\n\\n\"))\n",
    "\n",
    "        \n",
    "gen_seq(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
