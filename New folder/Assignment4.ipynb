{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    " %tensorflow_version 2.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data : (50000, 32, 32, 3)\n",
      "Test Data : (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print('Train Data : {}'.format(x_train.shape))\n",
    "print('Test Data : {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape([-1, 32, 32, 3]).astype(np.float32) / 255, y_train.astype(np.int32)))\n",
    "data = data.shuffle(buffer_size=60000)\n",
    "data = data.batch(128)\n",
    "data = data.repeat()\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test.reshape([-1, 32, 32, 3]).astype(np.float32) / 255, y_test.astype(np.int32)))\n",
    "test_data = test_data.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 1000\n",
    "inp = tf.keras.layers.Input((32, 32, 3))\n",
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_3/Identity:0' shape=(None, 32, 32, 16) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense Block 1\n",
    "# layer 1\n",
    "layer1_d1 = tf.keras.layers.BatchNormalization()(inp)\n",
    "layer1_d1 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer1_d1)\n",
    "layer1_d1 = tf.keras.layers.BatchNormalization()(layer1_d1)\n",
    "layer1_d1 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer1_d1)\n",
    "conc_layer1_d1 = tf.keras.layers.concatenate([inp, layer1_d1], axis=-1)\n",
    "\n",
    "# layer 2\n",
    "layer2_d1 = tf.keras.layers.BatchNormalization()(conc_layer1_d1)\n",
    "layer2_d1 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer2_d1)\n",
    "layer2_d1 = tf.keras.layers.BatchNormalization()(layer2_d1)\n",
    "layer2_d1 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer2_d1)\n",
    "conc_layer12_d1 = tf.keras.layers.concatenate([layer1_d1, layer2_d1], axis=-1)\n",
    "\n",
    "# layer 3\n",
    "layer3_d1 = tf.keras.layers.BatchNormalization()(conc_layer12_d1)\n",
    "layer3_d1 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer3_d1)\n",
    "layer3_d1 = tf.keras.layers.BatchNormalization()(layer3_d1)\n",
    "layer3_d1 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer3_d1)\n",
    "conc_layer13_d1 = tf.keras.layers.concatenate([layer1_d1, layer2_d1, layer3_d1], axis=-1)\n",
    "\n",
    "# layer 4\n",
    "layer4_d1 = tf.keras.layers.BatchNormalization()(conc_layer13_d1)\n",
    "layer4_d1 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer4_d1)\n",
    "layer4_d1 = tf.keras.layers.BatchNormalization()(layer4_d1)\n",
    "layer4_d1 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer4_d1)\n",
    "conc_layer14_d1 = tf.keras.layers.concatenate([layer1_d1, layer2_d1, layer3_d1, layer4_d1], axis=-1)\n",
    "\n",
    "# Transition Block 1\n",
    "tb1 = tf.keras.layers.BatchNormalization()(conc_layer14_d1)\n",
    "tb1 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(tb1)\n",
    "tb1 = tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='same')(tb1)\n",
    "\n",
    "\n",
    "# Dense Block 2\n",
    "# layer 1\n",
    "layer1_d2 = tf.keras.layers.BatchNormalization()(tb1)\n",
    "layer1_d2 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer1_d2)\n",
    "layer1_d2 = tf.keras.layers.BatchNormalization()(layer1_d2)\n",
    "layer1_d2 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer1_d2)\n",
    "conc_layer1_d2 = tf.keras.layers.concatenate([tb1, layer1_d2], axis=-1)\n",
    "\n",
    "# layer 2\n",
    "layer2_d2 = tf.keras.layers.BatchNormalization()(conc_layer1_d2)\n",
    "layer2_d2 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer2_d2)\n",
    "layer2_d2 = tf.keras.layers.BatchNormalization()(layer2_d2)\n",
    "layer2_d2 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer2_d2)\n",
    "conc_layer12_d2 = tf.keras.layers.concatenate([layer1_d2, layer2_d2], axis=-1)\n",
    "\n",
    "# layer 3\n",
    "layer3_d2 = tf.keras.layers.BatchNormalization()(conc_layer12_d2)\n",
    "layer3_d2 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer3_d2)\n",
    "layer3_d2 = tf.keras.layers.BatchNormalization()(layer3_d2)\n",
    "layer3_d2 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer3_d2)\n",
    "conc_layer13_d2 = tf.keras.layers.concatenate([layer1_d2, layer2_d2, layer3_d2], axis=-1)\n",
    "\n",
    "# layer 4\n",
    "layer4_d2 = tf.keras.layers.BatchNormalization()(conc_layer13_d2)\n",
    "layer4_d2 = tf.keras.layers.Conv2D(filters=4*k, kernel_size=(1,1), padding='same', strides=1, activation=tf.nn.relu)(layer4_d2)\n",
    "layer4_d2 = tf.keras.layers.BatchNormalization()(layer4_d2)\n",
    "layer4_d2 = tf.keras.layers.Conv2D(filters=k, kernel_size=(3,3), padding='same', strides=1, activation=tf.nn.relu)(layer4_d2)\n",
    "conc_layer14_d2 = tf.keras.layers.concatenate([layer1_d2, layer2_d2, layer3_d2, layer4_d2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense/Identity:0' shape=(None, 10) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global Average Pooling\n",
    "output = tf.keras.layers.GlobalAveragePooling2D()(conc_layer14_d2)\n",
    "output = tf.keras.layers.Flatten()(output)\n",
    "output = tf.keras.layers.Dense(10)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inp, output)\n",
    "model.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[tf.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1000 steps, validate for 79 steps\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 797s 797ms/step - loss: 0.9199 - sparse_categorical_accuracy: 0.6703 - val_loss: 0.9682 - val_sparse_categorical_accuracy: 0.6496\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1412s 1s/step - loss: 0.8862 - sparse_categorical_accuracy: 0.6830 - val_loss: 0.9704 - val_sparse_categorical_accuracy: 0.6505\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 937s 937ms/step - loss: 0.8573 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.9898 - val_sparse_categorical_accuracy: 0.6448\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 980s 980ms/step - loss: 0.8330 - sparse_categorical_accuracy: 0.7018 - val_loss: 0.9128 - val_sparse_categorical_accuracy: 0.6707\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 872s 872ms/step - loss: 0.8137 - sparse_categorical_accuracy: 0.7091 - val_loss: 0.9291 - val_sparse_categorical_accuracy: 0.6729\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data, steps_per_epoch=1000, epochs=5, validation_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
